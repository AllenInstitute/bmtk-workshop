{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V1 Neural Network Training Tutorial (Preview)\n",
        "\n",
        "\n",
        "This notebook demonstrates how to train a biologically realistic V1 neural network model that simulates the primary visual cortex. The model incorporates key biological features like spiking neurons, realistic connectivity patterns, and physiological response properties. Through this tutorial, you'll learn how to:\n",
        "\n",
        "- Configure and initialize a V1 network model\n",
        "- Set up training parameters and loss functions\n",
        "- Train the model using visual stimuli\n",
        "- Evaluate the model's responses and tuning properties\n",
        "- Analyze and visualize the results\n",
        "\n",
        "Note: This functionality is not currently included within BMTK. This tutorial provides a preview of planned future capabilities for training biologically realistic V1 networks.\n",
        "\n",
        "\n",
        "\n",
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from time import time\n",
        "\n",
        "# Import our tutorial support functions\n",
        "import v1_tutorial_support as vts\n",
        "from importlib import reload\n",
        "reload(vts)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set up the training parameters. Modify these for different experiments:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create configuration object\n",
        "config = vts.TutorialConfig()\n",
        "\n",
        "# Tutorial settings (smaller scale for demonstration)\n",
        "config.n_epochs = 10\n",
        "config.steps_per_epoch = 10\n",
        "config.neurons = 5000  # Use subset for faster demo\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  Epochs: {config.n_epochs}\")\n",
        "print(f\"  Steps per epoch: {config.steps_per_epoch}\")\n",
        "print(f\"  Neurons: {config.neurons} (0 = all)\")\n",
        "print(f\"  Learning rate: {config.learning_rate}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Configure TensorFlow environment for optimal performance:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environment\n",
        "vts.setup_environment(config)\n",
        "dtype = vts.configure_mixed_precision(config.dtype)\n",
        "strategy = vts.create_distribution_strategy()\n",
        "logdir, flag_str = vts.prepare_directories(config)\n",
        "\n",
        "print(f\"Results will be saved to: {logdir}\")\n",
        "\n",
        "# Calculate batch sizes\n",
        "per_replica_batch_size = config.batch_size\n",
        "global_batch_size = per_replica_batch_size * strategy.num_replicas_in_sync\n",
        "print(f\"Batch sizes: {per_replica_batch_size} per replica, {global_batch_size} global\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Network\n",
        "\n",
        "Load the pre-built V1 network with realistic connectivity:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load network data\n",
        "network, lgn_input, bkg_input = vts.load_network_data(config, flag_str)\n",
        "delays = [200, 0]  # [pre_delay, post_delay] in ms\n",
        "\n",
        "print(f\"Network loaded with {network['n_nodes']} neurons\")\n",
        "print(f\"Stimulus timing: pre={delays[0]}ms, post={delays[1]}ms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Model and Optimizer\n",
        "\n",
        "Build the neural network model and configure training:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model within strategy scope\n",
        "with strategy.scope():\n",
        "    model = vts.create_model(\n",
        "        config, network, lgn_input, bkg_input, \n",
        "        dtype, per_replica_batch_size\n",
        "    )\n",
        "    \n",
        "    optimizer = vts.create_optimizer(config, model, config.dtype)\n",
        "    \n",
        "    print(f\"Model created with {model.count_params():,} parameters\")\n",
        "    print(f\"Optimizer: {type(optimizer).__name__}\")\n",
        "    \n",
        "    # Store initial weights for comparison\n",
        "    initial_weights = [var.numpy().copy() for var in model.trainable_variables[:2]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Loss Functions\n",
        "\n",
        "Configure the biologically-motivated loss functions:\n",
        "\n",
        "- Rate loss: Matches the firing rate distribution of the model to experimental data\n",
        "- Voltage loss: Regularizes membrane potentials to stay within biologically plausible ranges\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    # Setup spatial masks and loss functions\n",
        "    core_mask, annulus_mask = vts.setup_core_masks(config, network)\n",
        "    loss_components = vts.create_loss_functions(\n",
        "        config, model, network, dtype, delays, core_mask, logdir\n",
        "    )\n",
        "    metrics, reset_metrics = vts.create_training_metrics()\n",
        "    \n",
        "    print(\"Loss functions configured:\")\n",
        "    print(\"✓ Rate distribution matching\")\n",
        "    print(\"✓ Voltage regularization\") \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Input Data Generation by LGN\n",
        "\n",
        "Create datasets for visual stimuli:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset functions\n",
        "get_gratings_dataset_fn, _ = vts.create_dataset_functions(\n",
        "    config, global_batch_size, dtype, delays\n",
        ")\n",
        "train_data_set = strategy.distribute_datasets_from_function(get_gratings_dataset_fn())\n",
        "\n",
        "# Precompute spontaneous firing rates\n",
        "spontaneous_prob = vts.compute_spontaneous_lgn_rates(config, dtype)\n",
        "print(f\"Data pipeline ready, spontaneous rates: {spontaneous_prob.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Training Functions\n",
        "\n",
        "Create the core training step function. Within this function, it runs simulation,\n",
        "calculates loss function terms, calculate gradients, and apply them to the weights.\n",
        "\n",
        "Note: @tf.function() is used to convert Python functions into TensorFlow graphs, which significantly speeds up execution by avoiding Python's overhead and enabling optimizations. This is especially important for training models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tf.function()  # accelerate training by using tf.function\n",
        "def simple_training_step(x, y):\n",
        "    \"\"\"Simplified training step for tutorial\"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass through model\n",
        "        rsnn_layer = loss_components['rsnn_layer']\n",
        "        dummy_zeros = tf.zeros((per_replica_batch_size, tf.shape(x)[1], network[\"n_nodes\"]), dtype)\n",
        "        zero_state = rsnn_layer.cell.zero_state(per_replica_batch_size, dtype)\n",
        "        \n",
        "        extractor_model = tf.keras.Model(inputs=model.inputs, outputs=rsnn_layer.output)\n",
        "        out = extractor_model((x, dummy_zeros, zero_state))\n",
        "        z, v = out[0]  # spikes, voltages\n",
        "        \n",
        "        # Compute losses (simplified for tutorial)\n",
        "        rate_loss = loss_components['evoked_rate_regularizer'](z, True)\n",
        "        voltage_loss = loss_components['voltage_regularizer'](v)\n",
        "        \n",
        "        total_loss = rate_loss + voltage_loss\n",
        "    \n",
        "        scaled_loss = total_loss\n",
        "    \n",
        "    # Compute and apply gradients\n",
        "    gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    return total_loss, z, rate_loss, voltage_loss\n",
        "\n",
        "print(\"Training functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "Now you are ready to run training. It will take a few minutes to finish the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize training history\n",
        "training_history = {\n",
        "    'epochs': [], 'loss': [], 'firing_rate': [], \n",
        "    'rate_loss': [], 'voltage_loss': []\n",
        "}\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for epoch in range(config.n_epochs):\n",
        "    epoch_start = time()\n",
        "    epoch_losses, epoch_rates = [], []\n",
        "    epoch_rate_losses, epoch_voltage_losses, epoch_osi_losses = [], [], []\n",
        "    \n",
        "    # Create fresh dataset iterator\n",
        "    train_iterator = iter(train_data_set)\n",
        "    \n",
        "    for step in range(config.steps_per_epoch):\n",
        "        # Get training batch\n",
        "        x, y, _, _ = next(train_iterator)\n",
        "        \n",
        "        # Extract from distribution strategy if needed\n",
        "        if strategy.num_replicas_in_sync > 1:\n",
        "            x = strategy.experimental_local_results(x)[0]\n",
        "            y = strategy.experimental_local_results(y)[0]\n",
        "        \n",
        "        # Training step\n",
        "        loss, spikes, rate_loss, voltage_loss = simple_training_step(x, y)\n",
        "        \n",
        "        # Record metrics\n",
        "        epoch_losses.append(loss.numpy())\n",
        "        firing_rate = tf.reduce_mean(spikes).numpy() * 1000  # Convert to Hz\n",
        "        epoch_rates.append(firing_rate)\n",
        "        epoch_rate_losses.append(rate_loss.numpy())\n",
        "        epoch_voltage_losses.append(voltage_loss.numpy())\n",
        "        \n",
        "        if step % 2 == 0:\n",
        "            print(f\"  Step {step+1}: Loss={loss.numpy():.4f}, Rate={firing_rate:.2f} Hz\")\n",
        "    \n",
        "    # Epoch summary\n",
        "    epoch_time = time() - epoch_start\n",
        "    avg_loss = np.mean(epoch_losses)\n",
        "    avg_rate = np.mean(epoch_rates)\n",
        "    \n",
        "    # Store history\n",
        "    training_history['epochs'].append(epoch + 1)\n",
        "    training_history['loss'].append(avg_loss)\n",
        "    training_history['firing_rate'].append(avg_rate)\n",
        "    training_history['rate_loss'].append(np.mean(epoch_rate_losses))\n",
        "    training_history['voltage_loss'].append(np.mean(epoch_voltage_losses))\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch+1}: Loss={avg_loss:.4f}, Rate={avg_rate:.2f} Hz, Time={epoch_time:.1f}s\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"Training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Training Progress\n",
        "\n",
        "Plot the training curves:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "fig.suptitle('Training Progress', fontsize=16)\n",
        "\n",
        "# Total loss\n",
        "axes[0, 0].plot(training_history['epochs'], training_history['loss'], 'b-o', linewidth=2)\n",
        "axes[0, 0].set_title('Total Loss')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Firing rate\n",
        "axes[0, 1].plot(training_history['epochs'], training_history['firing_rate'], 'g-o', linewidth=2)\n",
        "axes[0, 1].set_title('Average Firing Rate')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Rate (Hz)')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Rate loss\n",
        "axes[1, 0].plot(training_history['epochs'], training_history['rate_loss'], 'r-o', linewidth=2)\n",
        "axes[1, 0].set_title('Rate Distribution Loss')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Rate Loss')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# OSI loss\n",
        "axes[1, 1].plot(training_history['epochs'], training_history['voltage_loss'], 'c-o', linewidth=2)\n",
        "axes[1, 1].set_title('Voltage Loss')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Voltage Loss')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Loss improved from {training_history['loss'][0]:.4f} to {training_history['loss'][-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Network Activity\n",
        "\n",
        "Examine how the network responds to visual stimuli:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test network response\n",
        "test_iterator = iter(train_data_set)\n",
        "x_test, y_test, _, _ = next(test_iterator)\n",
        "\n",
        "if strategy.num_replicas_in_sync > 1:\n",
        "    x_test = strategy.experimental_local_results(x_test)[0]\n",
        "    y_test = strategy.experimental_local_results(y_test)[0]\n",
        "\n",
        "# Get network response\n",
        "_, spikes, _, _ = simple_training_step(x_test, y_test)\n",
        "spikes_np = spikes.numpy()[0]  # First batch item\n",
        "\n",
        "# Analyze activity\n",
        "pop_rate = np.mean(spikes_np, axis=1) * 1000  # Convert to Hz\n",
        "mean_rate = np.mean(pop_rate)\n",
        "stim_start, stim_end = delays[0], len(pop_rate) - delays[1]\n",
        "stim_rate = np.mean(pop_rate[stim_start:stim_end])\n",
        "baseline_rate = np.mean(np.concatenate([pop_rate[:stim_start], pop_rate[stim_end:]]))\n",
        "\n",
        "print(f\"Network Activity:\")\n",
        "print(f\"  Average firing rate: {mean_rate:.2f} Hz\")\n",
        "print(f\"  Stimulus period firing rate: {stim_rate:.2f} Hz\")\n",
        "print(f\"  Spontaneous period firing rate: {baseline_rate:.2f} Hz\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max(pop_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Network Response\n",
        "\n",
        "Plot spike raster and population activity:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "# Spike raster (subset of neurons)\n",
        "n_neurons_plot = min(20000, spikes_np.shape[1])\n",
        "\n",
        "# pick neurons from masked region\n",
        "\n",
        "time_ms = np.arange(spikes_np.shape[0])\n",
        "# spike_times, spike_neurons = np.where(spikes_np[:, :n_neurons_plot] > 0.5)\n",
        "spike_times, spike_neurons = np.where(spikes_np > 0.5)\n",
        "\n",
        "ax1.scatter(spike_times, spike_neurons, s=1, alpha=0.7, c='red')\n",
        "ax1.set_title(f'Spike Raster ({n_neurons_plot} neurons)')\n",
        "ax1.set_ylabel('Neuron Index')\n",
        "ax1.axvline(delays[0], color='green', linestyle='--', alpha=0.7, label='Stimulus ON')\n",
        "ax1.axvline(len(time_ms) - delays[1], color='red', linestyle='--', alpha=0.7, label='Stimulus OFF')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Population firing rate\n",
        "ax2.plot(time_ms, pop_rate, 'blue', linewidth=1.5)\n",
        "ax2.set_title('Population Firing Rate')\n",
        "ax2.set_xlabel('Time (ms)')\n",
        "ax2.set_ylabel('Rate (Hz)')\n",
        "ax2.axvline(delays[0], color='green', linestyle='--', alpha=0.7, label='Stimulus ON')\n",
        "ax2.axvline(len(time_ms) - delays[1], color='red', linestyle='--', alpha=0.7, label='Stimulus OFF')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weight Change Analysis\n",
        "\n",
        "Examine how synaptic weights changed during training:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze weight changes\n",
        "final_weights = [var.numpy() for var in model.trainable_variables[:2]]\n",
        "weight_changes = []\n",
        "\n",
        "for initial, final in zip(initial_weights, final_weights):\n",
        "    change = np.mean(np.abs(final - initial) / (np.abs(initial) + 1e-8))\n",
        "    weight_changes.append(change)\n",
        "\n",
        "layer_names = [\"Recurrent\", \"Background\"]\n",
        "\n",
        "print(\"Weight Change Analysis:\")\n",
        "for i, change in enumerate(weight_changes):\n",
        "    print(f\"  {layer_names[i]}: {change:.4f} relative change\")\n",
        "\n",
        "# Plot weight change distribution\n",
        "if len(weight_changes) > 0:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(range(len(weight_changes)), weight_changes)\n",
        "    plt.title('Weight Changes by Layer')\n",
        "    plt.xlabel('Layer')\n",
        "    plt.ylabel('Relative Change')\n",
        "    plt.xticks(range(len(weight_changes)), [f'{layer_names[i]}' for i in range(len(weight_changes))])\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results\n",
        "\n",
        "Save the training results for future analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save training history\n",
        "import pickle\n",
        "with open(os.path.join(logdir, 'training_history.pkl'), 'wb') as f:\n",
        "    pickle.dump(training_history, f)\n",
        "\n",
        "# Save configuration\n",
        "with open(os.path.join(logdir, 'config.txt'), 'w') as f:\n",
        "    f.write(\"Tutorial Configuration:\\n\")\n",
        "    for attr in dir(config):\n",
        "        if not attr.startswith('_'):\n",
        "            f.write(f\"{attr}: {getattr(config, attr)}\\n\")\n",
        "\n",
        "print(f\"Results saved to: {logdir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "**Training Summary:**\n",
        "- Successfully trained a V1 network with biologically realistic constraints\n",
        "- Achieved convergence in firing rate distribution \n",
        "- Network demonstrates appropriate stimulus-evoked responses\n",
        "\n",
        "**Exercise**\n",
        "\n",
        "Let's add a term to the loss function:\n",
        "- Create a code section before the section defining 'simple_training_step' function\n",
        "- Copy the code block below into the new section\n",
        "- In simple_training_step, where losses are calculated, implement this new loss function and add it to the total loss\n",
        "- Modify the code to visualize how this loss function changes across training epochs\n",
        "- Evaluate whether this additional loss term meaningfully affects the resulting activity of the network\n",
        "- Analyze how other loss components (rate loss and voltage loss) change in response to this new term\n",
        "\n",
        "**Project Ideas**\n",
        "\n",
        "1. The network activity shows strong synchronization at gamma frequency. Consider:\n",
        "   - How might you modify the exercise's loss function to reduce such oscillations?\n",
        "   - Alternatively, could you design a loss function that encourages oscillations at a different frequency?\n",
        "\n",
        "2. Get creative and explore novel network behaviors\n",
        "\n",
        "Note: Different loss functions may require different training durations. Keep an eye on the loss trajectories to determine appropriate training lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def additional_loss_function(z):\n",
        "    \"\"\" Sample loss function that penalize activity of the neurons with specific IDs.\n",
        "\n",
        "        z: a tensor of shape (batch_size, seq_len, n_neurons) that contains the spikes.\n",
        "    \"\"\"\n",
        "\n",
        "    # Here is a simple example of a loss function that penalize activity of the neurons\n",
        "    # with specific IDs. Let's say we want to suppress neuron 0-500, 1000-1500, and so on.\n",
        "    penalized_ids = np.concatenate([np.arange(i, i+500) for i in range(0, 5000, 1000)])\n",
        "\n",
        "    # Convert input to float32 tensor if not already\n",
        "    z = tf.cast(z, tf.float32)\n",
        "\n",
        "    # now we will calculate the loss as the mean of the activity of the penalized neurons.\n",
        "    # be sure to use tf.gather so that it can be compiled with tf.function.\n",
        "    loss = tf.reduce_mean(tf.gather(z, penalized_ids, axis=2))\n",
        "    \n",
        "    return loss * 10000  # multiply a coefficient to make it as large as other losses."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
