{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Small Network Model with PointNet\n",
    "\n",
    "In the last tutorial we built a toy model of the Mouse Visual Cortex system with 75 biophysically detailed neurons, that we ran with BioNet (i.e., the NEURON simulator). This time we will build/convert a similar model but instead using PointNet (i.e., the NEST simulator) with point-neuron-based models. In particular, we will use the Generalized Leaky Integrate-and-Fire (GLIF) models created at the Allen Institute and for which we can find optimized models in the Allen Cell-Types Database.\n",
    "\n",
    "As we will see the process for building and simulating point-neuron models are similar to biophysically detailed neurons, with the appropriate adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from distutils.dir_util import copy_tree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nest\n",
    "\n",
    "from bmtk.builder.networks import NetworkBuilder\n",
    "from bmtk.simulator import pointnet\n",
    "from bmtk.analyzer.compartment import plot_traces\n",
    "from bmtk.analyzer.spike_trains import plot_raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Models\n",
    "\n",
    "As before we will first want to find appropiate models using the [Allen Cell-Types Database Feature Search](https://celltypes.brain-map.org/data). We choose 3 different spiny excitatory models (identified by expression of Scnn1a, Rorb, and Nr5a1) and one inhibitory model (identified by expression of PValb) for \"Mouse\" in the Location \"VISp\", \"L4\".\n",
    "\n",
    "This time we also want to make sure to check the box \"Has GLIF Model\".\n",
    "\n",
    "There are actually five different variations of the GLIF model, we are primarily interested in the \"type 3: LIF-ASC\" models. Choose that from the \"Select neuronal model\" dropdown, which will bring up the option to \"Download model\" at the bottom of the page:\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "  <img src=\"../images/cell_types_db_glif_selection_highlighted.png\" width=\"850\" align=\"left\" style=\"margin-left:4px\"/>    \n",
    "</div>\n",
    "<br clear=\"left\">\n",
    "\n",
    "The main file we are looking for is called *neuron_config.json*, which like *fit_parameters.json* for perisomatic models contains the **dynamics_params**. Rename _neuron_config.json_ to something more informative, then copy the file to the *components/point_neuron_models/* directory.\n",
    "\n",
    "As before we already did this - but feel free to choose your own models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip</b>: There is a small amount of biologically relevant randomness built in to the models and simulation, so everytime the notebook is run it is expected to get different results. However should you want to match the same results with the \"completed\" version of the tutorials please use the rng seed below:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "There are two ways we can build a 75 point-neuron version of the L4 model we created in the last chapter.\n",
    "\n",
    "1. <ins>Copy the morphologically detailed model and adjust parameters manually</ins> - If you already built the model you can just copy the files into the _network/_ directory. This way the cell-positions and connectivity matrix will be exactly the same (otherwise, due to calls to ```np.random```, each time we build the network it is a little different). A number of properties in the _node_types.csv_ and _edge_types.csv_ will need to be adjusted so that BMTK can work with our GLIF models. We can do it in a text editor or even programatically using pandas.\n",
    "\n",
    "\n",
    "2. <ins>Rebuild the model from scratch</ins> - This will involve using the same ```add_nodes()``` and ```add_edges()``` method we used in the previous chapter. Some properties, like **model_type**, **dynamics_params**, **syn_weight** will need to be adjusted to deal with the fact that we are using completely different models. Other properties like **morphology**, **afferent_section_id**, and **rotation_angle_\\*** can be removed completely as they won't apply to point-neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Copying an existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can copy the previous chapter's network files in the command line:\n",
    "\n",
    "```bash\n",
    "$ mkdir -p network_copied\n",
    "$ cp ../Ch3_multicells/network/* network_copied\n",
    "```\n",
    "\n",
    "Or in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "os.makedirs('network_copied', exist_ok=True)\n",
    "copy_tree('../Ch3_multicells/network', 'network_copied')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make the appropiate changes to the l4_nodes_types.csv, which we can do in a text editor or using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "node_types_df = pd.read_csv('network_copied/l4_node_types.csv', sep=' ')\n",
    "\n",
    "# remove unnecessary columns\n",
    "node_types_df = node_types_df.drop(columns=['model_processing', 'morphology', \n",
    "                                            'rotation_angle_xaxis'])\n",
    "# change model properties directive\n",
    "node_types_df['model_type'] = 'point_neuron'\n",
    "node_types_df['model_template'] = 'nest:glif_lif_asc_psc'\n",
    "node_types_df = node_types_df.replace({\n",
    "    'Scnn1a_485510712_params.json': 'Scnn1a_593618144_glif_lif_asc.json',\n",
    "    'Rorb_486509958_params.json': 'Rorb_480124551_glif_lif_asc.json',\n",
    "    'Nr5a1_485507735_params.json': 'Nr5a1_318808427_glif_lif_asc.json',\n",
    "    'Pvalb_473862421_params.json': 'Pvalb_487667205_glif_lif_asc.json'\n",
    "})\n",
    "\n",
    "# save \n",
    "node_types_df.to_csv('network_copied/l4_node_types.csv', sep=' ', index=False)\n",
    "\n",
    "node_types_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for et_path in ['network_copied/l4_l4_edge_types.csv', 'network_copied/lgn_l4_edge_types.csv']:\n",
    "    edge_types_df = pd.read_csv(et_path, sep=' ')\n",
    "    edge_types_df['model_template'] = 'static_synapse'\n",
    "    edge_types_df = edge_types_df.replace(\n",
    "        {'dynamics_params': 'AMPA|GABA'}, \n",
    "        {'dynamics_params': 'static'}, \n",
    "        regex=True\n",
    "    )\n",
    "\n",
    "    # Make it easier to change syn_weight by hand, move 'syn_weight' to end \n",
    "    syn_weight = edge_types_df.pop('syn_weight')\n",
    "    edge_types_df['syn_weight'] = '<TBD>'\n",
    "\n",
    "    edge_types_df.to_csv(et_path, sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will want to open our _edge_types.csv_ and modify the **syn_weight** column values. Readjusting synaptic weights when moving from biophysical models to point neuron models, or even between two different model types of the same level, can be very difficult. Although there are some tools in development which may help with parameter optimization of synaptic weights, like [nested](https://github.com/neurosutras/nested), the process often requires a lot of trial-and-error.\n",
    "\n",
    "We will often have to run multiple iterations of different types of simulations on individual neurons and the network and constantly update **syn_weight** until we can get expected firing rates:\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "  <img src=\"../images/update_syn_weights.png\" width=\"700\" align=\"left\" style=\"margin-left:4px\"/>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Rebuilding the model\n",
    "\n",
    "The following is a modification of the previous build script for our 75 L4 + 100 LGN network, but designed to run in PointNet. Some things to note:\n",
    "\n",
    "* The rules for generating coordinates and creating connections between cells (using tuning angle/distance) are the same as with the biophysical model.\n",
    "\n",
    "\n",
    "* We are no longer using the ```add_properties()``` method to find the sectional placement of the afferent synapse. We still need to know the number of synapses between a source/target pair of cells, but since the cells are just geometric \"points\" there is only one place we can put the synapses. \n",
    "\n",
    "\n",
    "* Our **model_type** will always be \"point_neuron\". For the nodes' **model_template** we can choose from a number of [available NEST cell models](https://nest-simulator.readthedocs.io/en/v3.2/models/index.html) or even build our own, although we will strictly be using the \"glif_psc\" model.\n",
    "\n",
    "\n",
    "* For our edges **model_template** we can also choose from one of the available [built-in synaptic models](https://nest-simulator.readthedocs.io/en/v3.2/models/index_synapse.html) or build our own. For simplicity we will be sticking with the standard \"static_synapse\" with the synaptic properties for each model saved in the _components/synaptic_models/_ directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_coords_column(N, radius_min=0.0, radius_max=400.0):\n",
    "    phi = 2.0 * np.pi * np.random.random([N])\n",
    "    r = np.sqrt((radius_min**2 - radius_max**2) * np.random.random([N]) + radius_max**2)\n",
    "    x = r * np.cos(phi)\n",
    "    y = np.random.uniform(400.0, 500.0, size=N)\n",
    "    z = r * np.sin(phi)\n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "def get_coords_plane(ncells, size_x=240.0, size_y=120.0):\n",
    "    xs = np.random.uniform(0.0, size_x, ncells)\n",
    "    ys = np.random.uniform(0.0, size_y, ncells)\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "def exc_exc_rule(source, target, max_syns):\n",
    "    \"\"\"Connect rule for exc-->exc neurons, should return an integer 0 or greater. The number of \n",
    "    connections will be weighted according to the difference between source and target cells' \n",
    "    tuning_angle property\"\"\"\n",
    "    if source['node_id'] == target['node_id']:\n",
    "        # prevent a cell from synapsing with itself\n",
    "        return 0\n",
    "    \n",
    "    # calculate the distance between tuning angles and use it to choose\n",
    "    # number of connections using a binomial distribution.\n",
    "    src_tuning = source['tuning_angle']\n",
    "    trg_tuning = target['tuning_angle']\n",
    "    tuning_dist = np.abs((src_tuning - trg_tuning + 180) % 360 - 180)\n",
    "    probs = 1.0 - (np.max((tuning_dist, 10.0)) / 180.0)\n",
    "    return np.random.binomial(n=max_syns, p=probs)\n",
    "\n",
    "\n",
    "def others_conn_rule(source, target, max_syns, max_distance=300.0, sigma=60.0):\n",
    "    \"\"\"Connection rule for exc-->inh, inh-->exc, or inh-->inh connections. The number of connections\n",
    "    will be based on the euclidian distance between source and target cell.\n",
    "    \"\"\"\n",
    "    if source['node_id'] == target['node_id']:\n",
    "        return 0\n",
    "    \n",
    "    dist = np.sqrt((source['x'] - target['x'])**2 + (source['z'] - target['z'])**2)\n",
    "    if dist > max_distance:\n",
    "        return 0\n",
    "    \n",
    "    prob = np.exp(-(dist/sigma)**2)\n",
    "    return np.random.binomial(n=max_syns, p=prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the recurrent (l4 --> l4) network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmtk.builder.networks import NetworkBuilder\n",
    "\n",
    "\n",
    "l4 = NetworkBuilder('l4')\n",
    "\n",
    "# Add nodes\n",
    "x, y, z = get_coords_column(20)\n",
    "l4.add_nodes(\n",
    "    N=20,   \n",
    "    model_type='point_neuron',\n",
    "    model_template='nest:glif_lif_asc_psc',\n",
    "    dynamics_params='Scnn1a_515806250_glif_lif_asc.json',   \n",
    "    x=x, y=y, z=z,\n",
    "    tuning_angle=np.linspace(start=0.0, stop=360.0, num=20, endpoint=False),\n",
    "    model_name='Scnn1a',\n",
    "    ei_type='e'\n",
    ")\n",
    "\n",
    "\n",
    "x, y, z = get_coords_column(20)\n",
    "l4.add_nodes(\n",
    "    N=20,\n",
    "    model_type='point_neuron',\n",
    "    model_template='nest:glif_lif_asc_psc',\n",
    "    dynamics_params='Rorb_512332555_glif_lif_asc.json',\n",
    "    x=x, y=y, z=z,   \n",
    "    model_name='Rorb', \n",
    "    ei_type='e',\n",
    "    tuning_angle=np.linspace(start=0.0, stop=360.0, num=20, endpoint=False),\n",
    ")\n",
    "\n",
    "x, y, z = get_coords_column(20)\n",
    "l4.add_nodes(\n",
    "    N=20,\n",
    "    model_type='point_neuron',\n",
    "    model_template='nest:glif_lif_asc_psc',\n",
    "    dynamics_params='Nr5a1_587862586_glif_lif_asc.json',   \n",
    "    x=x, y=y, z=z,    \n",
    "    model_name='Nr5a1', \n",
    "    ei_type='e',\n",
    "    tuning_angle=np.linspace(start=0.0, stop=360.0, num=20, endpoint=False),\n",
    ")\n",
    "\n",
    "x, y, z = get_coords_column(15)\n",
    "l4.add_nodes(\n",
    "    N=15,   \n",
    "    model_type='point_neuron',\n",
    "    model_template='nest:glif_lif_asc_psc',\n",
    "    dynamics_params='Pvalb_574058595_glif_lif_asc.json',\n",
    "    x=x, y=y, z=z, \n",
    "    model_name='PValb',\n",
    "    ei_type='i',\n",
    ")\n",
    "\n",
    "\n",
    "# Add recurrent edges\n",
    "l4.add_edges(\n",
    "    source=l4.nodes(ei_type='e'),\n",
    "    target=l4.nodes(ei_type='e'),    \n",
    "    connection_rule=exc_exc_rule,\n",
    "    connection_params={'max_syns': 15},\n",
    "    syn_weight=1.2,\n",
    "    delay=2.0,\n",
    "    dynamics_params='static_ExcToExc.json',\n",
    "    model_template='static_synapse',\n",
    ")\n",
    "\n",
    "l4.add_edges(\n",
    "    source=l4.nodes(ei_type='e'),\n",
    "    target=l4.nodes(ei_type='i'),\n",
    "    connection_rule=others_conn_rule,\n",
    "    connection_params={'max_syns': 12},\n",
    "    syn_weight=7.5,\n",
    "    delay=2.0,\n",
    "    dynamics_params='static_ExcToInh.json',\n",
    "    model_template='static_synapse',\n",
    ")\n",
    "\n",
    "l4.add_edges(\n",
    "    source=l4.nodes(ei_type='i'),\n",
    "    target=l4.nodes(ei_type='e'),\n",
    "    connection_rule=others_conn_rule,\n",
    "    connection_params={'max_syns': 14},\n",
    "    syn_weight=-10.5,\n",
    "    delay=2.0,\n",
    "    dynamics_params='static_InhToExc.json',\n",
    "    model_template='static_synapse',\n",
    ")\n",
    "\n",
    "l4.add_edges(\n",
    "    source=l4.nodes(ei_type='i'),\n",
    "    target=l4.nodes(ei_type='i'),\n",
    "    connection_rule=others_conn_rule,\n",
    "    connection_params={'max_syns': 14},\n",
    "    syn_weight=-2.0,\n",
    "    delay=2.0,\n",
    "    dynamics_params='static_InhToInh.json',\n",
    "    model_template='static_synapse',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l4.build()\n",
    "l4.save(output_dir='network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building feedforward  (lgn --> l4) inputs\n",
    "\n",
    "Now we can build our network of \"virtual\" LGN cells and connect them to the L4 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_lgn_cells(source, targets, max_targets, min_syns=1, max_syns=15, lgn_size=(240, 120),\n",
    "                      l4_radius=400.0, ellipse=(100.0, 500.0)):\n",
    "    # map the lgn cells from the plane to the circle\n",
    "    x, y = source['x'], source['y']\n",
    "    x = 2*(x / lgn_size[0] - 0.5)\n",
    "    y = 2*(y / lgn_size[1] - 0.5)\n",
    "    src_x = x * np.sqrt(1.0 - (y**2/2.0)) * l4_radius\n",
    "    src_y = y * np.sqrt(1.0 - (x**2/2.0)) * l4_radius\n",
    "    \n",
    "    # Find (the indices) of all the target cells that are within the given ellipse, if there are more than max_targets\n",
    "    # then randomly choose them\n",
    "    a, b = ellipse[0]**2, ellipse[1]**2\n",
    "    dists = [(src_x-t['x'])**2/a + (src_y-t['y'])**2/b for t in targets]\n",
    "    valid_targets = np.argwhere(np.array(dists) <= 1.0).flatten()\n",
    "    if len(valid_targets) > max_targets:\n",
    "        valid_targets = np.random.choice(valid_targets, size=max_targets, replace=False)\n",
    "\n",
    "    # create an array of all synapse counts. Most targets will have 0 connection, except for the \"valid_targets\" which\n",
    "    # which will have between [min_syns, max_syns] number of connections.\n",
    "    nsyns_arr = np.zeros(len(targets), dtype=int)\n",
    "    for idx in valid_targets:\n",
    "        nsyns_arr[idx] = np.random.randint(min_syns, max_syns)\n",
    "\n",
    "    return nsyns_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn = NetworkBuilder('lgn')\n",
    "\n",
    "# Build Nodes\n",
    "x, y = get_coords_plane(50)\n",
    "lgn.add_nodes(\n",
    "    N=50,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    model_type='virtual',\n",
    "    model_template='lgnmodel:tON_TF8',\n",
    "    dynamics_params='tON_TF8_demo.json',\n",
    "    ei_type='e'\n",
    ")\n",
    "\n",
    "x, y = get_coords_plane(50)\n",
    "lgn.add_nodes(\n",
    "    N=50,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    model_type='virtual',\n",
    "    model_template='lgnmodel:tOFF_TF8',\n",
    "    dynamics_params='tOFF_TF8_demo.json',\n",
    "    ei_type='e'\n",
    ")\n",
    "\n",
    "# Build Edges\n",
    "lgn.add_edges(\n",
    "    source=lgn.nodes(),\n",
    "    target=l4.nodes(ei_type='e'),\n",
    "    connection_rule = connect_lgn_cells,\n",
    "    connection_params = {'max_targets': 6},\n",
    "    iterator='one_to_all',\n",
    "    model_template='static_synapse',\n",
    "    dynamics_params='static_ExcToExc.json',\n",
    "    delay=2.0,\n",
    "    syn_weight=11.0\n",
    ")\n",
    "\n",
    "lgn.add_edges(\n",
    "    source=lgn.nodes(),\n",
    "    target=l4.nodes(ei_type='i'),\n",
    "    connection_rule=connect_lgn_cells,\n",
    "    connection_params={'max_targets': 12, 'ellipse': (400.0, 400.0)},\n",
    "    iterator='one_to_all',\n",
    "    model_template='static_synapse',\n",
    "    dynamics_params='static_ExcToInh.json',\n",
    "    delay=2.0,\n",
    "    syn_weight=13.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn.build()\n",
    "lgn.save(output_dir='network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use VND to examine the cells in this model\n",
    "\n",
    "Launch VND by typing `vnd` on the Linux command line.\n",
    "\n",
    "In the Visual Neuronal Dynamics window select, select menu item  **File : Open File with Edges**\n",
    "and choose the file `Ch4_pointnet/config.pointnet.json`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the Main tab, in the Representations pane:\n",
    "\n",
    "In the list of representations, after loading, the single, default selection is:\n",
    "\n",
    "- Selected Neurons: all\n",
    "- Coloring Method: Type\n",
    "- Style: soma\n",
    "\n",
    "\n",
    "\n",
    "<div>\n",
    "  <img src=\"../images/ch4_all_soma_persp.png\" width=\"805\" align=\"left\" style=\"margin-left:26px\"/>    \n",
    "</div>\n",
    "<br clear=\"left\">\n",
    "\n",
    "In the Connections tab, create an \"all\" to \"all\" \"simple_edge\" representation.\n",
    "Then create an \"all\" to \"all\" connection representation style source_soma with color green spheres and another with style target_soma with color red.  The connections above the main disc are to the virtual LGN neurons (green spheres).  The LGN locations are arbitrarirly confined to a rectangle hovering above the model, but their location has no actual meaning in this model.\n",
    "\n",
    "Explore the synapse locations with zooming, rotation, and translation.\n",
    "\n",
    "\n",
    "<div>\n",
    "  <img src=\"../images/ch4_all_all_conn.png\" width=\"805\" align=\"left\" style=\"margin-left:26px\"/>    \n",
    "</div>\n",
    "<br clear=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below VND image, the LGN cells are shown in yellow composing a 2D rectangle above the plane of the l4 and lif cells. The position in space of the set of LGN cells in this image is arbitrary, and only for convenient visual reference. Two of the LGN (source) cells are highlighted in green, their target l4 cells arer highlighted in red, with silver cylinders connecting source and target.  For each LGN source cell, the set of target l4 cells was defined by the connection rule function `connection_lgn_cells` determined by an ellipse.\n",
    "\n",
    " <div>\n",
    "  <img src=\"../images/ch4_two_lgn_edges_to_l4_rev_c.png\" width=\"805\" align=\"left\" style=\"margin-left:26px\"/>    \n",
    "</div>\n",
    "<br clear=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs for LGN\n",
    "\n",
    "As before we need to provide inputs for our LGN \"virtual\" cells. We could use the ```bmtk.utils.reports.spike_trains.PoissonSpikeGenerator``` object as was done before. But since the spike files are the same for BioNet and PointNet we can just copy the existing lgn-spikes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "os.makedirs('inputs', exist_ok=True)\n",
    "shutil.copy('../Ch3_multicells/inputs/lgn_spikes.h5', 'inputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment\n",
    "\n",
    "As before we need to initialize the simulation directory, including configuration files, python run scripts, and the model file components. We've already set this up for you. But if running a PointNet simulation from scratch the following can help using the ```bmtk.utils.create_environment``` helper functions - either through the command line:\n",
    "\n",
    "```bash\n",
    "  $ python -m bmtk.utils.create_environment     \\\n",
    "       --config-file config.pointnet.json       \\\n",
    "       --network-dir network                    \\\n",
    "       --output-dir output_pointnet             \\\n",
    "       --tstop 3000.0                           \\\n",
    "       --dt 0.1                                 \\\n",
    "       --report-vars l4:V_m                     \\\n",
    "       --spikes-inputs lgn:inputs/lgn_spikes.h5 \\\n",
    "       --run-script run_pointnet.py             \\\n",
    "       --overwrite                              \\\n",
    "       pointnet .\n",
    "```\n",
    "\n",
    "or through python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmtk.utils.create_environment import create_environment\n",
    "\n",
    "create_environment(\n",
    "    'pointnet',\n",
    "    base_dir='.',\n",
    "    config_file='config.pointnet.json',\n",
    "    network_dir='network',\n",
    "    output_dir='output_pointnet',\n",
    "    tstop=3000.0, dt=0.1,\n",
    "    report_vars=[('l4', 'V_m')],\n",
    "    spikes_inputs=[('lgn', 'inputs/lgn_spikes.h5')],\n",
    "    run_script='run_pointnet.py',\n",
    "    # overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Simulation\n",
    "\n",
    "We can run our GLIF version of the L4 network through the command line:\n",
    "\n",
    "```bash\n",
    "$ python run_pointnet.py config.pointnet.json\n",
    "```\n",
    "\n",
    "We can use MPI to run the simulation on multiple cores:\n",
    "```bash\n",
    "$ mpirun -np <N> python run_pointnet.py config.pointnet.json\n",
    "```\n",
    "\n",
    "or to run it in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmtk.simulator import pointnet\n",
    "\n",
    "configure = pointnet.Config.from_json('config.pointnet.json')\n",
    "configure.build_env()\n",
    "\n",
    "network = pointnet.PointNetwork.from_config(configure)\n",
    "sim = pointnet.PointSimulator.from_config(configure, network)\n",
    "sim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmtk.analyzer.spike_trains import plot_raster\n",
    "\n",
    "_ = plot_raster(config_file='config.pointnet.json', group_by='model_name', show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show pointnet spike activity animation in VND\n",
    "\n",
    "Launch VND by typing `vnd` on the Linux command line.\n",
    "\n",
    "In the Visual Neuronal Dynamics window select, select menu item  **File : Open File with Edges**\n",
    "and choose the file `Ch4_pointnet/config.pointnet.json'\n",
    "\n",
    "\n",
    "In the Main tab, in the Representations pane:\n",
    "\n",
    "In the list of representations, after loading, the   single, default selection is:\n",
    "\n",
    "- Selected Neurons: all\n",
    "- Coloring Method: Type\n",
    "- Style: soma\n",
    "\n",
    "Select menu item  **File : Add File with Spikes** and choose `Ch4_pointnet/output_pointnet/spikes.h5`\n",
    "\n",
    "In the Activity window, set\n",
    "\n",
    "- Population:l4\n",
    "\n",
    "Click **Update Selection**\n",
    "\n",
    "set:\n",
    "- Color: blue\n",
    "- Sphere Scale: 10\n",
    "- Sphere Resolution: 5\n",
    "- Step: 5\n",
    "- Time window: 10\n",
    "\n",
    "If no sturctures are showing, you may need to select **Display : Reset View**.  (Or even, in the Main tab Representations pane,select the soma, \"all\" representation, then select **Display : Reset View**.)\n",
    "\n",
    "Drag the time slider left and right ,  drag it back to the far left, then press the right-pointing play button to play through the series of spikes.  Change speed with the Speed slider.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct link to <a href=\"http://www.ks.uiuc.edu/~barryi/bmtk_allen_workshop_2022/ch4_pointnet_l4_spikes.mp4\n",
    "\"> pointnet l4 spikes video </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "So far we've been using randomized spike trains to drive our ```BioNet``` and ```PointNet``` toy models of the mouse VISp cortical system. But what if we want to use more realistic stimuli that would be expect to come from thalamic input? What if we want to be able to test and compare our in silico model with the in vivo results found in the [Allen Brain Observatory](https://observatory.brain-map.org/visualcoding/)? In the next chapter will demonstrate how to generate realistic, optimized stimuli using the ```FilterNet``` module.\n",
    "\n",
    "\n",
    "[**Proceed to Chapter 5**](../Ch5_filternet/5.%20FilterNet.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [OPTIONAL] PopNet\n",
    "\n",
    "We've already built a version of our L4 model using multi-compartment biophysically-detailed cells in NEURON (BioNet) and point-neuron GLIF-based models in NEST (PointNet). BMTK includes a third simulator engine called PopNet which can be used to simulate population level firing rates using stabilized supralinear networks. For a brief tutorial on building the PopNet version of our toy visual system, please see [4.1 PopNet SNN](../Ch4.1_popnet/4.1%20PopNet%20SSN.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Proceed to Chapter 5**](../Ch5_filternet/5.%20FilterNet.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmtk-py3.11",
   "language": "python",
   "name": "bmtk-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
