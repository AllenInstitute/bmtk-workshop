{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0477c1f-ba3f-48bf-9510-c95a59f49da4",
   "metadata": {},
   "source": [
    "# Advanced methods for driving your network with synaptic spike-trains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacc6a76-0928-42dd-9257-009a3a628810",
   "metadata": {},
   "source": [
    "BMTK is designed with the goal of building and simulating large-scale, realistic models of the nervous system. An important part for most of these realistic simulations is being able to reacreate the kinds of synaptic input you would expect to see in-vivo. And in order to achieve that modelers need to be able to generate the relevant spike-trains that drive the synpatic stimuli of our simulations.\n",
    "\n",
    "We have other tutorials that demonstrate how to generate realistic stimli using [FilterNet](../Ch5_filternet/5.%20FilterNet.ipynb). And we have also shown how to use the BMTK [SpikeTrain and PoissonSpikeTrain](https://alleninstitute.github.io/bmtk/tutorials/tutorial_03_single_pop.html#Spike-Trains) class to generate input spike-trains. In these cases it requires users generate SONATA spike-train files before running their simulations. But BMTK also always different methods for users to generate and import spike-train stimuli into their simulations; including using other formats besides SONATA, importing experimental data, as-well-as more fine-grained control of how and where cells within a network recive synaptic stimuli.\n",
    "\n",
    "We will cover some of such methods in the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ef55a-c1ee-4173-932f-998f916672b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Contents\n",
    "1. [Example: Spike-train inputs from a custom created csv file](#example-spikes-from-csv)\n",
    "2. [Example: Dynamically generating spike-trains with cusom function](#example-spikes-from-func)\n",
    "3. [Example: Incorporating in-vivo NWB spiking data into your simulation](#example-spikes-from-nwb)\n",
    "4. [Example: Forcing spotonaneous synaptic activity within a network](#example-spikes-from-spont)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7cd5bb-39cb-4a78-99f1-0db8e4b15d19",
   "metadata": {},
   "source": [
    "## Example: Using spikes from CSV <a class=\"anchor\" id=\"example-spikes-from-csv\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0fbbe4-28eb-466d-9173-05072374f05e",
   "metadata": {},
   "source": [
    "In [Chapter 2](../Ch2_single_cell/2.%20Single%20Cell.ipynb#poisson-spike-generator) we demonstrated how to generate network spikes using the BMTK's built in PoissonSpikeGenerator class, which will produce a series of spike trains using the distribution parameters/functions we give it and save it to a SONATA spikes file using to `to_sonata` method. But BMTK also allows for input spikes to be saved and loaded using a simplier space-separated csv file, which in many cases can be easier to analyze and use with other programs.\n",
    "\n",
    "For example in the **network_simple_spikes/** folder (built using `build_network.simple_spikes.py`) we have a set of 10 virtual nodes that are used to synaptically drive our simulation. If we wanted each one to fire at a constant 10Hz firing rate over a 5 second interval we can use `PoissonSpikeGenerator`'s `to_csv()` class to save the spikes as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa014a-4da1-47ae-aa2c-1a71f4e7ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmtk.utils.reports.spike_trains import PoissonSpikeGenerator\n",
    "\n",
    "psg = PoissonSpikeGenerator()\n",
    "psg.add(\n",
    "    node_ids='network_csv_spikes/inputs_nodes.h5', \n",
    "    firing_rate=10.0, \n",
    "    times=(0.0, 5.0),\n",
    "    population='inputs'\n",
    ")\n",
    "psg.to_csv('inputs/simple_spikes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9a62f1-46e3-406b-87a3-0cdd6f02f72a",
   "metadata": {},
   "source": [
    "Now we can use any text editor or csv reader (like pandas) to read in our spikes for verification and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85348f1-2095-43aa-a986-2988b2968433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spikes_inputs = pd.read_csv('inputs/simple_spikes.csv', sep=' ')\n",
    "\n",
    "# Let's quickly check that our csv file makes sense\n",
    "n_nodes = spikes_inputs['node_ids'].unique().shape[0]\n",
    "n_spikes = spikes_inputs.shape[0]\n",
    "\n",
    "print(f'Number of cells in spikes-file: {n_nodes} (expected: 5)')\n",
    "print(f'Avg. number of spikes per cell: {n_spikes/n_nodes} (expected: 5sec x 10Hz ~ 50)')\n",
    "print(f'Min spike-time: {spikes_inputs[\"timestamps\"].min()} ms')\n",
    "print(f'Max spike-time: {spikes_inputs[\"timestamps\"].max()} ms')\n",
    "\n",
    "spikes_inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aca5aa-2136-4651-bbed-66981c132591",
   "metadata": {},
   "source": [
    "Then when running the simulation we can do so as we previous did with SONATA spike files but with the difference is that in the **module** type for the given input must be changed from `sonata` to `csv`:\n",
    "\n",
    "```json\n",
    "  \"inputs\": {\n",
    "    \"csv_spikes\": {\n",
    "      \"input_type\": \"spikes\",\n",
    "      \"module\": \"csv\",\n",
    "      \"input_file\": \"./inputs/simple_spikes.csv\",\n",
    "      \"node_set\": \"inputs\"\n",
    "    }\n",
    "  },"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efd9ab-888c-4b52-8caf-38f4744d2ba5",
   "metadata": {},
   "source": [
    "### Creating your own csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655dbe88-d071-48cc-a691-2802ef66a809",
   "metadata": {},
   "source": [
    "If you don't want to use the `PoissonSpikeGenerator` class you can also create your own spike-train csv file. As we can see from above, it needs to be a space-separated text file with columns **timestamps**, **population**, and **node_ids** (order doesn't matter). Each row indicates a separate spike, with the **population** + **node_ids** columns indicating the node/cell that fired, and **timestamps** (in milliseconds) indicating when it fired.\n",
    "\n",
    "For example, we can use the python csv writer class to create an example input file where each cell has increasing firing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0e902-dd4c-4468-ac43-a97bf372d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "with open('inputs/custom_spikes.csv', 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=' ', quotechar='#')\n",
    "\n",
    "    # write the header\n",
    "    csvwriter.writerow(['timestamps', 'population', 'node_ids'])\n",
    "    \n",
    "    # For node 0 we have it fire randomly at 1 Hz for 5 seconds, for node 1 at 2Hz, etc.\n",
    "    for node_id in range(10):\n",
    "        for timestamp in np.sort(np.random.uniform(0.0, 5000.0, size=(node_id+1)*5)):\n",
    "            csvwriter.writerow([timestamp, 'inputs', node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f03cf-18dc-4e88-985f-275c6ce4cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmtk.analyzer.spike_trains import plot_raster\n",
    "\n",
    "_ = plot_raster(spikes_file='inputs/custom_spikes.csv', with_histogram=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf47f53-740f-44b3-b167-aec0c3117b4c",
   "metadata": {},
   "source": [
    "We can now run the simulation with our custom csv input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836b12f-9f91-40d2-9031-5e1a927245ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmtk.simulator import bionet\n",
    "from bmtk.analyzer.spike_trains import plot_raster, to_dataframe\n",
    "\n",
    "bionet.reset()\n",
    "conf = bionet.Config.from_json('config.csv_spikes.json')\n",
    "conf.build_env()\n",
    "\n",
    "net = bionet.BioNetwork.from_config(conf)\n",
    "sim = bionet.BioSimulator.from_config(conf, network=net)\n",
    "sim.run()\n",
    "\n",
    "_ = plot_raster(config_file='config.csv_spikes.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76baf2be-28b2-4944-981b-d7e12fad5578",
   "metadata": {},
   "source": [
    "## Example: Dynamically generating custom spike-trains <a class=\"anchor\" id=\"example-spikes-from-func\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1860a4-4702-4e6c-b839-5d4372acd95b",
   "metadata": {},
   "source": [
    "Having to pregenerate spike-trains for you simulations inside a hdf5 or CSV file is computationally efficent and makes your results easier to reproduce and share. However, at times it may be beneificial for users to generate spike-trains dynamically during each simulation. If you are doing quick simulation and spot-checks it can be cumbersome having to regenerate a file beforehand. Or if a user is running thousand of simulations, or doing some kind of gradient search, the cost of having to create potentially thousand of spike files beforehand may not be reasonable.\n",
    "\n",
    "BMTK also allows modlers the option to create their own special function which will generate new spike-trains at the start of each simulation. To do so, you only need to make changes to the configuration for a \"spikes\" input so that the **module** is set to value `function`, and instead of spikes_input you specify a **spikes_function** parameter that will be the name of your custom function\n",
    "\n",
    "```json\n",
    "\"inputs\": {\n",
    "    \"LGN_spikes_sonata\": {\n",
    "        \"input_type\": \"spikes\",\n",
    "        \"module\": \"function\",\n",
    "        \"spikes_function\": \"my_spikes_generator\",\n",
    "        \"node_set\": \"LGN\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a677788-8993-4612-b595-51c4bff5ba30",
   "metadata": {},
   "source": [
    "For BMTK to know where to find the `my_spikes_generator` you must then use the `@spikes_generator` at the top of a given function in your run_bmtk.py script (or any python file imported into run script). In the below example we use the following to return spike trains for every cell in the `LGN` node_set to fire at a constant rate (although different rates for differnt models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014e00c-b6b8-42c9-a37b-13df612ff431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmtk.simulator.bionet.io_tools import io\n",
    "from bmtk.simulator.bionet import spikes_generator\n",
    "import numpy as np\n",
    "\n",
    "@spikes_generator\n",
    "def my_spikes_generator(node, sim):\n",
    "    io.log_info(f'Generating custom spike trains for {node.node_id} from node {node.population_name}')\n",
    "    if node['pop_name'] == 'tON':\n",
    "        return np.arange(100.0, sim.tstop, step=sim.dt*10)\n",
    "    elif node['pop_name'] == 'tOFF':\n",
    "        return np.arange(100.0, sim.tstop, step=sim.dt*20)\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f07bf1-fc89-46cb-b23f-35fe33c0a041",
   "metadata": {},
   "source": [
    "* All `spikes_generator` functions must have parameters `node` and `sim`\n",
    "  * `node` allow you to access information about each node/cell like a dictionary.\n",
    "  * `sim` is a class containing information about a current simulation, including information like start time (`sim.tstart`), stop_time (`sim.tstop`), step size (`sim.dt`) among other properties.\n",
    "* The `spikes_generator` should return either a list or array of timestamps, in milliseconds, for each node.\n",
    "* For good measure, we include logging statement. This is not required, but is good for debugging and making sure our custom function is being called.\n",
    "\n",
    "When the simulation is ran the `my_spikes_generator` function will be called once for each node in the specified **node_set**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36732033-c4f4-4c36-994c-b2ec5ef44c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmtk.simulator import bionet\n",
    "\n",
    "bionet.reset()\n",
    "conf = bionet.Config.from_json('config.spikes_generator.json')\n",
    "conf.build_env()\n",
    "\n",
    "net = bionet.BioNetwork.from_config(conf)\n",
    "sim = bionet.BioSimulator.from_config(conf, network=net)\n",
    "sim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fbd76-66df-405e-acb2-57a843711405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce4e0aca-0600-4b82-87f0-151f168e2de9",
   "metadata": {},
   "source": [
    "## Example: Incorporating real data from NWB files <a class=\"anchor\" id=\"example-spikes-from-nwb\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a55bc-a592-4778-9f6d-0862da876721",
   "metadata": {},
   "source": [
    "While you can use tools like FilterNet or PoissonSpikeGenerator to create theoretical-to-realistic synaptic stimuli onto a network from a variety of different modes. But better yet, when actual experimental data is available, it is often possible to use just that. Especially as as more-and-more experimental electrophysiological data sets are being made publically available through resources like [DANDI](https://dandiarchive.org/) or [The Allen Brain Observatory](https://portal.brain-map.org/circuits-behavior/visual-coding-neuropixels) we will want to not only use such data as a base-line for validating our models and comparing them to experiments, but also to use the data within specific simulations. For example, when modeling a network of one population and/or region, we will want use recordings of surrounding cells to help excite and inhibit our cells in a more realistic manner.\n",
    "\n",
    "Traditionally a major issue with incorporating experimental electrophysiology data into simulations is trying to parse the wide variety of different ways the data was stored. Luckily, the [Neurodata Without Borders (NWB)](https://www.nwb.org/) have developed a format for storing experimental data which has seen a substantial amount of adoption in the field. BMTK can take these files and automatically insert them into simulations.\n",
    "\n",
    "In this example we will take the previous model of the Mouse Primary Visual Cortex and add inputs that we know come from higher cortical regions (in this case just the VisL) along with the input from the LGN. For these added regions, we will use actual Neuropixels recordings of activity from these regions during presentation of drifting gratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7df1db-fa11-45e9-ba96-f803a2e7d210",
   "metadata": {},
   "source": [
    "### Step 1: Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d183688f-c1fd-477d-ad34-cb91106b0b9b",
   "metadata": {},
   "source": [
    "First step is to find experimental NWB that includes spiking events. For our example we will use data from [Allen Institute Visual Coding dataset](https://observatory.brain-map.org/visualcoding/) downloaded using the AllenSDK. We will get a three experimental sessions that we know contains recordings the the VisL and hippocamus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79d70f-b028-4e17-8991-26f54ae130db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allensdk.brain_observatory.ecephys.ecephys_project_cache import EcephysProjectCache\n",
    "\n",
    "cache = EcephysProjectCache.from_warehouse(\n",
    "    manifest='./ecephys_cache_dir/neuropixels.manifest.json'\n",
    ")\n",
    "cache.get_session_data(715093703)\n",
    "cache.get_session_data(798911424)\n",
    "cache.get_session_data(754829445)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc957f1-abf5-4f0e-8304-89f207db88e4",
   "metadata": {},
   "source": [
    "By defaul the NWB files will be downloaded into the *ecephys_cache_dir*. Since nwb files are essentially just structured HDF5 files you can use tools like [HDFView](https://www.hdfgroup.org/downloads/hdfview/) or [h5py](https://www.h5py.org/) to read them once they have been downloaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99daefa-2859-491c-8226-2edc181e8afd",
   "metadata": {},
   "source": [
    "### Step 2: Connecting VISL and Hippocampal onto our V1 cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba9ede7-b667-4038-ba9a-3f46076e8a1e",
   "metadata": {},
   "source": [
    "To simulation synaptic stimulation from the VisL cell recordings onto our V1 model, we will need to create a population of virtual cells representing the VisL neurons and their synapses to V1. Unforantely the electrophyiology data doesn't include information about network geometry and connectivity, so it is up to the modeler to decide how to connection the experimental data into our network.\n",
    "\n",
    "As we did before we will separate node populations called 'VISl' and use the NetworkBuilder to create feedforward synaptic connections\n",
    "\n",
    "```python\n",
    "visl = NetworkBuilder('VISl')\n",
    "visl.add_nodes(\n",
    "    N=n_visl_units,\n",
    "    model_type='virtual',\n",
    "    ...\n",
    ")\n",
    "visl.add_edges(\n",
    "    source=visal.nodes(),\n",
    "    target=visp.nodes(ei='e'), \n",
    "    connection_rule=connection_rule_e2e,\n",
    "    dynamics_params='AMPA_ExcToExc.json',\n",
    "    model_template='Exp2Syn',\n",
    "    ...\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "See the *./build_network.nwb_inputs.py* for the full script that builds the SONATA network found in *./network_nwb_inputs/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d7297f-d370-44c4-aa80-eb111367c040",
   "metadata": {},
   "source": [
    "### Step 3: Updating the configuration file to include NWB data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598b866-32db-4f70-974d-fe7d59abe2e7",
   "metadata": {},
   "source": [
    "Before we can run the simulation we must update the SONATA configuration file so that the simulation:\n",
    "1. Knows which .nwb files to fetch spiking data from\n",
    "2. Knows how to map cells (eg. NWB units) from our experimental data to cells (eg. SONATA nodes) in our 'VISl' and 'hippocampus' populations.\n",
    "3. Know which interval interval in the experimental data to use in our simulation.\n",
    "\n",
    "The most straight forward way of doing this is to add the following to our configuration file (*config.nwb_inputs.json*) in the \"inputs\" section:\n",
    "\n",
    "```json\n",
    "\"inputs\": {\n",
    "    \"hippo_spikes\": {\n",
    "        \"input_type\": \"spikes\",\n",
    "        \"module\": \"ecephys_probe\",\n",
    "        \"node_set\": \"hippocampus\",\n",
    "        \"input_file\": \"./ecephys_cache_dir/session_715093703/session_715093703.nwb\",\n",
    "        \"units\": {\n",
    "            \"location\": [\"CA1\", \"CA3\", \"Po\"]\n",
    "        }\n",
    "        \"mapping\": \"sample\",\n",
    "        \"interval\": {\n",
    "            \"interval_name\": \"drifting_gratings\",\n",
    "            \"temporal_frequency\": 4.0,\n",
    "            \"orientation\": 90\n",
    "        }\n",
    "\n",
    "    },\n",
    "    \"visl_spikes\": {\n",
    "        \"input_type\": \"spikes\",\n",
    "        \"module\": \"ecephys_probe\",\n",
    "        \"node_set\": \"VISl\",\n",
    "        \"input_file\": \"./ecephys_cache_dir/session_715093703/session_715093703.nwb\",\n",
    "        \"mapping\": \"sample\",\n",
    "        \"units\": {\n",
    "            \"location\": \"VISl\",\n",
    "        }\n",
    "        \"interval\": {\n",
    "            \"interval_name\": \"drifting_gratings\",\n",
    "            \"temporal_frequency\": 4.0,\n",
    "            \"orientation\": 90\n",
    "        },\n",
    "    }\n",
    "}\n",
    "```\n",
    "* The **input_type** and **module** will always be set to values `spikes` and `ecephys_probe`, respectively, when importing extracellular electrophysiology NWB files into your simulation.\n",
    "* The **node_set** is the subset of cells in our network to use as virtual cells that are generating spikes.\n",
    "* The **input_file** in the name of the nwb file(s) to use for spikes. To use data from multiple sessions just use a list of files:\n",
    "```json\n",
    "    \"input_file\": [\n",
    "        \"./ecephys_cache_dir/session_715093703/session_715093703.nwb\",\n",
    "        \"./ecephys_cache_dir/session_798911424/session_798911424.nwb\",\n",
    "        \"./ecephys_cache_dir/session_754829445/session_754829445.nwb\"\n",
    "    ]\n",
    "```\n",
    "* The **units** field tell us which units from the nwb file to take their spiking data from based on either specifc keywords and/or unit-id. In the Neuropixels NWB files each unit has an field called \"location\" to determine which region the data came from, which we can use here to tell that for our model's \"hippocampus\" cells use any data coming from either the CA1, CA3 or Po regions.\n",
    "* **mapping** tells how to map NWB **units** -> SONATA **node_set**. Setting the value to `sample` will result in random mapping without replacement. You can also use options `sample_with_replacment`, useful if you have more nodes in your model than units in your data. Or if you have a specific mapping from NWB unit_ids to SONATA node_ids you can use optionn `units_map` (in which case **units** will point to a csv file).\n",
    "* **interval** tells simulation which interval of time to fetch spikes from. Inside the NWB file there is a stimulus table that marks the stimuli at any given epoch of time. We use this table to only get spikes recorded which a \"drifting grating\" stimuli was present with a given orientation and frequency. If you know the specific time you can also use option\n",
    "```json\n",
    "    \"interval\": [start_time_ms, end_time_ms]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2aadb1-2d31-4d26-8574-591229cd570c",
   "metadata": {},
   "source": [
    "And finally we are ready to run our simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bfceb-64f0-4cd1-91a8-83aac4acfc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmtk.simulator import bionet\n",
    "\n",
    "bionet.reset()\n",
    "conf = bionet.Config.from_json('config.nwb_inputs.json')\n",
    "conf.build_env()\n",
    "\n",
    "net = bionet.BioNetwork.from_config(conf)\n",
    "sim = bionet.BioSimulator.from_config(conf, network=net)\n",
    "sim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64971d-41b2-4cab-afd2-a309f04863bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "448dfd35-f226-471c-98e5-929d5bc71571",
   "metadata": {},
   "source": [
    "## Example: Forcing spotonaneous synaptic activity within a network <a class=\"anchor\" id=\"example-spikes-from-spont\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18e4ca-aba8-4f41-ab52-8e19bfebf6c4",
   "metadata": {},
   "source": [
    "So far in this tutorials we've focused on generating stimuli using synaptic stimuli coming from outside our main modeled network. In other tutorials we should different types in input to drive a simulation including current-clamps, voltage-clamps, and extracellar stimulation. While these can be generate both the kinds of stimuli we might see in the experiments and living brains, they tend to not be very grainular, especially when we want to study the secondary effects of activity within a network.\n",
    "\n",
    "One option that BMTK gives use for having more grainular control of internal network activity is by forcing certain synapses to spontaneously fire at pre-determined times. Not only can this give us more control of network dynamics that would be much harder to achive using current clamps or feedforward spike trains. But it also let's us isolate external activity from recurrent activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463087a0-380e-459a-9abb-d70d659e94b2",
   "metadata": {},
   "source": [
    "In BMTK this is done by adding a new input type to the \"inputs\" section the SONATA config with **input_type** and **module** called `syn_activity`. And the minimum we must define the pre-synaptic cells that will spontaneously fire and a list of firing times:\n",
    "\n",
    "```json\n",
    "\"syn_activity\": {\n",
    "  \"input_type\": \"syn_activity\",\n",
    "  \"module\": \"syn_activity\",\n",
    "  \"precell_filter\": {\n",
    "      \"population\": \"VISp\",\n",
    "      \"ei\": \"e\"\n",
    "  },\n",
    "  \"timestamps\": [500.0, 1000.0, 1500.0, 2000.0, 2500.0, 3000.0, 3500.0]\n",
    "}\n",
    "```\n",
    "* **precell_filter** determines the synapses to sponataneously activate based on the presynaptic/source cell. In this case we tell BMTK spontaneous activity to apply to synapses with a source-cell that has attributes `population==VISp` and `ei==e`. If you know exactly which cells you want to use you can filter by `node_id`:\n",
    "```json\n",
    "    \"node_id\": [0, 1, 2, 3],\n",
    "```\n",
    "* **timestamps** is a list of timestamps, in milliseconds, to activate the neuron following startup. If you have too many timestamps to add to the json directly, you can also pass in a string path to a txt file where each line is a timestamp.\n",
    "\n",
    "\n",
    "In the above example, all the synapses with VISp, exc pre-synaptic connections will fire at the given timestamp. For further grainular control you can all set the **postcell_filter** too for filtering out synapses based on post-synaptic cell. For example if you want spontaneous firing in exc -> inh connections (the above example would also include exc -> exc synapses:\n",
    "\n",
    "```json\n",
    "\"syn_activity\": {\n",
    "  \"input_type\": \"syn_activity\",\n",
    "  \"module\": \"syn_activity\",\n",
    "  \"precell_filter\": {\n",
    "      \"population\": \"VISp\",\n",
    "      \"ei\": \"e\"\n",
    "  },\n",
    "  \"postcell_filter\": {\n",
    "      \"population\": \"VISp\",\n",
    "      \"ei\": \"i\"\n",
    "  },\n",
    "  \"timestamps\": [500.0, 1000.0, 1500.0, 2000.0, 2500.0, 3000.0, 3500.0]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957112fc-f8ae-4841-95dd-0a0bd81be884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bmtk.simulator import bionet\n",
    "\n",
    "bionet.reset()\n",
    "conf = bionet.Config.from_json('config.spont_syns.json')\n",
    "conf.build_env()\n",
    "\n",
    "net = bionet.BioNetwork.from_config(conf)\n",
    "sim = bionet.BioSimulator.from_config(conf, network=net)\n",
    "sim.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd66b3-53a0-479b-bbc4-af03a704028a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bmtk-py3.10",
   "language": "python",
   "name": "bmtk-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
